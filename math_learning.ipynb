{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1409d685-658f-471c-882f-1e37498c49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa150a1b-ab25-41f9-9944-3a43d6ae68ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch' from 'C:\\\\ProgramData\\\\anaconda3\\\\Lib\\\\site-packages\\\\torch\\\\__init__.py'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef028f0-5f42-42b9-8428-1c073097752f",
   "metadata": {},
   "source": [
    "# 反向传播的一些理解\n",
    " 反向传播的深入理解 https://gengzhige-essay.readthedocs.io/docs/03%20%E6%84%9F%E7%9F%A5%E6%9C%BA/3-3%20%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3.html\n",
    "\n",
    "[梯度下降和反向传播算法数据科学家指南](https://developer.nvidia.com/zh-cn/blog/a-data-scientists-guide-to-gradient-descent-and-backpropagation-algorithms/) \n",
    "\n",
    "\n",
    "[一文弄懂神经网络中的反向传播法——BackPropagation](https://www.cnblogs.com/charlotte77/p/5629865.html)\n",
    "\n",
    "\n",
    "在反向传播过程中,每一行数据(或每一个批次的数据)都会用来更新所有的权重(weights)。这是神经网络学习的核心机制。具体来说:\r\n",
    "\r\n",
    "1. 前向传播: 输入数据通过网络,计算预测输出。\r\n",
    "\r\n",
    "2. 计算损失: 比较预测输出和实际标签,得到误差。\r\n",
    "\r\n",
    "3. 反向传播: 误差从输出层向输入层反向传播,计算每个权重对误差的贡献(梯度)。\r\n",
    "\r\n",
    "4. 更新权重: 使用优化算法(如梯度下降)根据计算出的梯度更新所有权重。\r\n",
    "\r\n",
    "这个过程对每一行数据(在批量处理中是每一批数据)都会重复进行。因此:\r\n",
    "\r\n",
    "- 在随机梯度下降(SGD)中,每处理一行数据就更新一次所有权重。\r\n",
    "- 在小批量梯度下降中,每处理一个批次的数据就更新一次所有权重。\r\n",
    "- 在批量梯度下降中,处理完所有数据后更新一次所有权重。\r\n",
    "\r\n",
    "这种频繁的更新允许模型逐步学习数据中的模式,不断调整以最小化预测误差。\r\n",
    "\r\n",
    "需要注意的是,虽然每次都更新所有权重,但每个权重的更新幅度可能不同,这取决于该权重对当前误差的贡献程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2474687-8845-48ce-840e-62b430e2689d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db3fcd7d-24af-49fb-a6a7-fdbe71dec1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor and set requires_grad=True to track computations\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Define a simple function\n",
    "y = x**2 + 3*x + 1\n",
    "\n",
    "# Compute the gradients\n",
    "y.backward()\n",
    "\n",
    "# Print the gradient of y with respect to x\n",
    "print(x.grad)  # This will output the gradient dy/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a5e2d3-63d9-4a6c-bcfe-572fcc886442",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "20588b84-2be5-4032-9cf3-5053269350c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]),\n",
       " torch.Size([2, 1]),\n",
       " tensor(7.5400, grad_fn=<MulBackward0>),\n",
       " torch.Size([]),\n",
       " torch.Tensor)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Step 1: Create a tensor for input (x) and target output (y_true)\n",
    "x = torch.tensor([[3.0, 2.0, 1.0],[1.3,2.4,3.7]])  # Input feature with 3 dimensions\n",
    "y_true = torch.tensor([[5.0],[2.0]])  # Target output\n",
    "\n",
    "# Step 2: Initialize parameters (weight and bias)\n",
    "w = torch.tensor([[1.0], [1.0], [1.0]], requires_grad=True)  # Weight for 3 features\n",
    "b = torch.tensor([[0.0]], requires_grad=True)  # Bias\n",
    "\n",
    "# Step 3: Define the learning rate\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Step 4: Forward pass\n",
    "y_pred = x @ w + b  # Linear model: y = wx + b\n",
    "\n",
    "# Step 5: Calculate the loss (Mean Squared Error)\n",
    "loss = 0.5 * (y_pred - y_true).pow(2).mean()  # MSE loss\n",
    "y_pred.shape, y_true.shape, loss,loss.shape, type((y_pred - y_true).pow(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "def5256f-bf40-4144-be44-81fb0fa89363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of loss w.r.t weight: tensor([[ 5.0100],\n",
      "        [ 7.4800],\n",
      "        [10.4900]])\n",
      "Gradient of loss w.r.t bias: 3.200000286102295\n",
      "Updated weight: tensor([[ 0.4990],\n",
      "        [ 0.2520],\n",
      "        [-0.0490]], requires_grad=True)\n",
      "Updated bias: -0.320000022649765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1]),\n",
       " torch.Size([2, 1]),\n",
       " tensor(7.5400, grad_fn=<MulBackward0>),\n",
       " torch.Size([]))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 6: Backward pass\n",
    "loss.backward()  # Compute gradients\n",
    "\n",
    "# Step 7: Print gradients\n",
    "print(f\"Gradient of loss w.r.t weight: {w.grad}\")  # Print the entire gradient tensor\n",
    "print(f\"Gradient of loss w.r.t bias: {b.grad.item()}\")  # Print bias gradient as scalar\n",
    "\n",
    "# Step 8: Update parameters (weights and bias)\n",
    "with torch.no_grad():  # Disable gradient tracking for updates\n",
    "    w -= learning_rate * w.grad  # Update weight\n",
    "    b -= learning_rate * b.grad  # Update bias\n",
    "\n",
    "# Step 9: Zero the gradients after updating\n",
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "\n",
    "# Step 10: Print updated parameters\n",
    "print(f\"Updated weight: {w}\")  # Print the entire weight tensor\n",
    "print(f\"Updated bias: {b.item()}\")  # Print bias as scalar\n",
    "\n",
    "y_pred.shape, y_true.shape, loss,loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5ee71039-1fea-46c4-b570-d6721038efa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.5890\n",
      "Epoch [4/10], Loss: 0.5890\n",
      "Epoch [6/10], Loss: 0.5890\n",
      "Epoch [8/10], Loss: 0.5890\n",
      "Epoch [10/10], Loss: 0.5890\n",
      "预测值:\n",
      "tensor([[-0.7204],\n",
      "        [-0.8232],\n",
      "        [-0.4771],\n",
      "        [-0.2774],\n",
      "        [-0.3740],\n",
      "        [-0.2620],\n",
      "        [-0.5669],\n",
      "        [-0.4316],\n",
      "        [-0.2425],\n",
      "        [-0.7539]])\n",
      "实际值:\n",
      "tensor([[-0.0499],\n",
      "        [ 0.5263],\n",
      "        [-0.0085],\n",
      "        [ 0.7291],\n",
      "        [ 0.1331],\n",
      "        [ 0.8640],\n",
      "        [-1.0157],\n",
      "        [-0.8887],\n",
      "        [ 0.1498],\n",
      "        [-0.2089]])\n",
      "tensor([[-0.6705],\n",
      "        [-1.3496],\n",
      "        [-0.4686],\n",
      "        [-1.0065],\n",
      "        [-0.5072],\n",
      "        [-1.1260],\n",
      "        [ 0.4488],\n",
      "        [ 0.4572],\n",
      "        [-0.3923],\n",
      "        [-0.5450]])\n"
     ]
    }
   ],
   "source": [
    "# 神经网络的极简例子\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置随机种子以确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 创建模拟数据\n",
    "# 10条数据，每条4个特征\n",
    "X = torch.randn(10, 4)\n",
    "# 创建目标值（这里我们假设是回归问题，每个样本对应一个目标值）\n",
    "y = torch.randn(10, 1)\n",
    "\n",
    "# 定义神经网络模型\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 8)  # 输入层到隐藏层\n",
    "        self.fc2 = nn.Linear(8, 1)  # 隐藏层到输出层\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# 实例化模型、损失函数和优化器\n",
    "model = SimpleNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "# printmodel.parameters()）\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    outputs = model(X) # 前向传播 - 计算最后的输出值\n",
    "    loss = criterion(outputs, y) # 根据损失计算函数去计算当前损失\n",
    "    \n",
    "    # 反向传播和优化\n",
    "    optimizer.zero_grad() # 重置梯度为0\n",
    "    loss.backward() # 反向传播 - 反向传播会计算每个权重的偏导数（权重对于结果的影响）\n",
    "    optimizer.step() # 这里才是更新权重值\n",
    "    \n",
    "    # 打印每100个epoch的损失\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 测试模型\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(X)\n",
    "    print(\"预测值:\")\n",
    "    print(test_output)\n",
    "    print(\"实际值:\")\n",
    "    print(y)\n",
    "    print(test_output-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0608116-c516-466a-99c1-d579cf173e84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
